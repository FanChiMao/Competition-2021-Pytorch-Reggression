{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMBD2021-MLP regression.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ptIRp1GUFzZKqgbjW02DRAhRqPf6wjGe","authorship_tag":"ABX9TyMOVhPp12VFvJn/FFXv9K/U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"moip0Ehh6g1T","executionInfo":{"status":"ok","timestamp":1629985553003,"user_tz":-480,"elapsed":426,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["# model utils -> 和模型相關的函式\n","import torch\n","import os\n","from collections import OrderedDict\n","def freeze(model):\n","    for p in model.parameters():\n","        p.requires_grad = False\n","def unfreeze(model):\n","    for p in model.parameters():\n","        p.requires_grad = True\n","def is_frozen(model):\n","    x = [p.requires_grad for p in model.parameters()]\n","    return not all(x)\n","def save_checkpoint(model_dir, state, session):\n","    epoch = state['epoch']\n","    model_out_path = os.path.join(model_dir, \"model_epoch_{}_{}.pth\".format(epoch, session))\n","    torch.save(state, model_out_path)\n","def load_checkpoint(model, weights):\n","    checkpoint = torch.load(weights)\n","    try:\n","        model.load_state_dict(checkpoint[\"state_dict\"])\n","    except:\n","        state_dict = checkpoint[\"state_dict\"]\n","        new_state_dict = OrderedDict()\n","        for k, v in state_dict.items():\n","            name = k[7:]  # remove `module.`\n","            new_state_dict[name] = v\n","        model.load_state_dict(new_state_dict)\n","def load_start_epoch(weights):\n","    checkpoint = torch.load(weights)\n","    epoch = checkpoint[\"epoch\"]\n","    return epoch\n","def load_optim(optimizer, weights):\n","    checkpoint = torch.load(weights)\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # for p in optimizer.param_groups: lr = p['lr']\n","    # return lr\n","def network_parameters(nets):\n","    num_params = sum(param.numel() for param in nets.parameters())\n","    return num_params   # /1e6\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"slnM8tul8VoQ","executionInfo":{"status":"ok","timestamp":1629985553004,"user_tz":-480,"elapsed":5,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["# dir -> 和路徑有關的函式\n","import os\n","from natsort import natsorted\n","from glob import glob\n","\n","\n","def mkdirs(paths):\n","    if isinstance(paths, list) and not isinstance(paths, str):\n","        for path in paths:\n","            mkdir(path)\n","    else:\n","        mkdir(paths)\n","\n","\n","def mkdir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\n","def get_last_path(path, session):\n","    x = natsorted(glob(os.path.join(path, '*%s' % session)))[-1]\n","    return x\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSzcigCNVC78","executionInfo":{"status":"ok","timestamp":1629985553007,"user_tz":-480,"elapsed":8,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["# csv -> 和csv有關的函式\n","import csv\n","import os\n","\n","\n","def write_csv(data=None, csv_path=None, save_name='result'):\n","    if not os.path.exists(csv_path):\n","        os.mkdir(csv_path)\n","    headerList = ['Number', 'Predict']\n","    with open(csv_path + \"/\" + save_name + '.csv', 'w', newline='') as f:\n","        w = csv.writer(f)\n","        dw = csv.DictWriter(f, delimiter=',', fieldnames=headerList)\n","        dw.writeheader()\n","        for i in range(len(data)):\n","            name = data[i][0]\n","            predict = data[i][1]\n","            w.writerow([name, predict])\n","    f.close()\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpOFRpIGVMAs","executionInfo":{"status":"ok","timestamp":1629985553434,"user_tz":-480,"elapsed":434,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["# score -> 和分數計算有關的函式\n","\n","def calculate_score_A(x):\n","    return x * 70\n","\n","\n","def calculate_score_B(y):\n","    score_B = 0\n","    if y <= 5:\n","        score_B = 30\n","    elif 5 < y <= 7.5:\n","        score_B = 25\n","    elif 5 < y <= 7.5:\n","        score_B = 20\n","    elif 7.5 < y <= 10:\n","        score_B = 25\n","    elif 10 < y <= 12.5:\n","        score_B = 17.5\n","    elif 12.5 < y <= 15:\n","        score_B = 15\n","    elif 15 < y <= 17.5:\n","        score_B = 10\n","    elif 15 < y <= 17.5:\n","        score_B = 5\n","    elif y > 20:\n","        score_B = 0\n","    return score_B\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hA5hiJeV63Ks","executionInfo":{"status":"ok","timestamp":1629985553729,"user_tz":-480,"elapsed":298,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["# dataset.py -> 整理data，並換為可訓練資料(Normalization)\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import StandardScaler\n","\n","# dataset definition\n","class TrainDataset(Dataset):\n","\n","    # load the dataset\n","    def __init__(self, train_path=None):\n","        # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","        train_out = pd.read_csv(train_path)\n","        inputs = train_out.iloc[:, 1:14].values\n","        outputs = train_out.iloc[:, 14].values\n","\n","        # feature scaling\n","        sc = StandardScaler()\n","        inputs_train = sc.fit_transform(inputs) # sc.fit_transform()\n","        outputs_train = outputs\n","\n","\n","        # 轉成張量(tensor)\n","        self.inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n","        self.outputs_train = torch.tensor(outputs_train, dtype=torch.float32).view(-1, 1)\n","\n","    def __len__(self):\n","        return len(self.outputs_train)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs_train[idx], self.outputs_train[idx]\n","\n","\n","class ValDataset(Dataset):\n","\n","    # load the dataset\n","    def __init__(self, train_path=None, val_path=None):\n","        # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","        train_out = pd.read_csv(train_path)\n","        train_total = train_out.iloc[:, 1:14].values\n","\n","        val_out = pd.read_csv(val_path)\n","        inputs = val_out.iloc[:, 1:14].values\n","        outputs = val_out.iloc[:, 14].values\n","\n","        # feature scaling\n","        sc = StandardScaler()\n","        matrix = sc.fit_transform(train_total)\n","        inputs_train = sc.transform(inputs)\n","        outputs_train = outputs\n","\n","        # 轉成張量(tensor)\n","        self.inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n","        self.outputs_train = torch.tensor(outputs_train, dtype=torch.float32).view(-1, 1)\n","\n","    class ValDataset(Dataset):\n","\n","        # load the dataset\n","        def __init__(self, train_path=None, val_path=None):\n","            # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","            train_out = pd.read_csv(train_path)\n","            train_total = train_out.iloc[:, 1:14].values\n","\n","            val_out = pd.read_csv(val_path)\n","            inputs = val_out.iloc[:, 1:14].values\n","            outputs = val_out.iloc[:, 14].values\n","\n","            # feature scaling\n","            sc = StandardScaler()\n","            matrix = sc.fit_transform(train_total)\n","            inputs_train = sc.transform(inputs)\n","            outputs_train = outputs\n","\n","            # 轉成張量(tensor)\n","            self.inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n","            self.outputs_train = torch.tensor(outputs_train, dtype=torch.float32).view(-1, 1)\n","\n","    def __len__(self):\n","        return len(self.outputs_train)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs_train[idx], self.outputs_train[idx]\n","\n","\n","class TestDataset(Dataset):\n","\n","    # load the dataset\n","    def __init__(self, train_path=None, test_path=None):\n","        # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","        train_out = pd.read_csv(train_path)\n","        train_total = train_out.iloc[:, 1:14].values\n","\n","        test_out = pd.read_csv(test_path)\n","        inputs = test_out.iloc[:, 1:14].values\n","        outputs = test_out.iloc[:, 14].values\n","        file_names = test_out.iloc[:, 0].values\n","\n","        # feature scaling\n","        sc = StandardScaler()\n","        matrix = sc.fit_transform(train_total)\n","        inputs_test = sc.transform(inputs)\n","        outputs_test = outputs\n","        names = file_names\n","\n","        # 轉成張量(tensor)\n","        self.inputs_test = torch.tensor(inputs_test, dtype=torch.float32)\n","        self.outputs_test = torch.tensor(outputs_test, dtype=torch.float32).view(-1, 1)\n","        self.file_name = torch.tensor(names, dtype=torch.int32).view(-1, 1)\n","\n","    def __len__(self):\n","        return len(self.inputs_test)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs_test[idx], self.outputs_test[idx], self.file_name[idx]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgMR8Jgm7aTM","executionInfo":{"status":"ok","timestamp":1629985553730,"user_tz":-480,"elapsed":5,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["# model.py -> 模型架構\n","import torch.nn as nn\n","\n","\n","# model definition\n","class MLP(nn.Module):\n","    # define model elements\n","    def __init__(self, n_inputs, hidden_layer1, hidden_layer2, hidden_layer3):\n","        super(MLP, self).__init__()\n","        self.layer_1 = nn.Linear(n_inputs, hidden_layer1)\n","        self.act1 = nn.PReLU()\n","        self.layer_2 = nn.Linear(hidden_layer1, hidden_layer2)\n","        self.act2 = nn.PReLU()\n","        self.layer_3 = nn.Linear(hidden_layer2, hidden_layer3)\n","        self.act3 = nn.PReLU()\n","        self.layer_4 = nn.Linear(hidden_layer3, 1)\n","        self.act4 = nn.LeakyReLU(0.2)\n","\n","    # forward propagate input\n","    def forward(self, x):\n","        x = self.act1(self.layer_1(x))\n","        x = self.act2(self.layer_2(x))\n","        x = self.act3(self.layer_3(x))\n","        x = self.act4(self.layer_4(x))\n","        return x\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJ6iTj5I7rW-","executionInfo":{"status":"ok","timestamp":1629985556943,"user_tz":-480,"elapsed":3217,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}},"outputId":"7fa202a6-b323-4fad-e830-96f98636445e"},"source":["# install tensorboardX\n","!pip install tensorboardx"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting tensorboardx\n","  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n","Installing collected packages: tensorboardx\n","Successfully installed tensorboardx-2.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p6PKTfTmPyKc","executionInfo":{"status":"ok","timestamp":1629985556943,"user_tz":-480,"elapsed":6,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["#----------------------Training parameter setting---------------------------#\n","\n","Network = 'MLP_128_256_128' #網路名字\n","EPOCH = 150\n","LR = 0.01\n","GPU = True\n","BATCH = 100\n","VAL_AFTER_EVERY = 5 #每幾個epoch存一次模型\n","TRAIN_DIR = '/content/train.csv' # path to training data\n","VAL_DIR = '/content/val.csv' # path to validation data\n","SAVE_DIR = '/content' # path to save models and images\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wYxSJ4pu6I0g","executionInfo":{"status":"error","timestamp":1629985643935,"user_tz":-480,"elapsed":86997,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}},"outputId":"d81c9309-a5d9-4a18-c2e9-a77c57db3055"},"source":["#--------------------------------Start training !----------------------------#\n","import yaml\n","import os\n","import torch\n","\n","torch.backends.cudnn.benchmark = True\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","import random\n","import time\n","import numpy as np\n","from tqdm import tqdm\n","from tensorboardX import SummaryWriter\n","\n","## Set Seeds\n","random.seed(1234)\n","np.random.seed(1234)\n","torch.manual_seed(1234)\n","torch.cuda.manual_seed_all(1234)\n","\n","## Model and log path direction\n","print('==> Build folder path')\n","start_epoch = 1\n","network_dir = os.path.join(SAVE_DIR, Network)\n","mkdir(network_dir)\n","save_dir = os.path.join(network_dir)\n","mkdir(save_dir)\n","model_dir = os.path.join(network_dir, 'models')\n","mkdir(model_dir)\n","log_dir = os.path.join(network_dir, 'log')\n","mkdir(log_dir)\n","train_dir = TRAIN_DIR\n","val_dir = VAL_DIR\n","\n","writer = SummaryWriter(log_dir=log_dir, filename_suffix=f'_log')\n","\n","# GPU device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Model\n","print('==> Build model')\n","model = MLP(n_inputs=13, hidden_layer1=128, hidden_layer2=128, hidden_layer3=64)\n","if GPU:\n","    model.to(device=device)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","# Loss function\n","criterion = nn.MSELoss()\n","\n","# DataLoaders\n","print('==> Data preparation')\n","train_dataset = TrainDataset(train_dir)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH,\n","                          shuffle=True, num_workers=0, drop_last=False, pin_memory=False)\n","val_dataset = ValDataset(train_dir, val_dir)\n","val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH, shuffle=False, num_workers=0,\n","                        drop_last=False, pin_memory=False)\n","\n","print(f'''==> Training details:\n","------------------------------------------------------------------------------\n","    Network:          {Network}\n","    Training data:      {len(train_dataset)}\n","    Validation data:    {len(val_dataset)}\n","    Start/End epochs:   {str(start_epoch) + '~' + str(EPOCH + 1)}\n","    Batch sizes:        {BATCH}\n","    Learning rate:      {LR}\n","    GPU:                {GPU}''')\n","print('------------------------------------------------------------------------------')\n","\n","# train\n","best_val_loss = 10000\n","best_epoch = 0\n","for epoch in range(start_epoch, EPOCH + 1):\n","    epoch_loss = 0\n","    model.train()\n","    for i, data in enumerate(tqdm(train_loader, ncols=70, total=len(train_loader), leave=True), 0):\n","\n","        for param in model.parameters():\n","            param.grad = None\n","\n","        inputs = data[0].cuda()\n","        GT = data[1].cuda()\n","        out = model(inputs)\n","        loss = criterion(out, GT)\n","\n","        # optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(' Epoch [{:3d}/{}]: \\tLoss: {:.4f}\\t'.format(epoch, EPOCH + 1, epoch_loss / len(train_loader)))\n","    print('------------------------------------------------------------------')\n","    torch.save({'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': optimizer.state_dict()\n","                }, os.path.join(model_dir, 'model_latest.pth'))\n","\n","    # validation (evaluation)\n","    if epoch % VAL_AFTER_EVERY == 0:\n","        model.eval()\n","        epoch_val_loss = 0\n","        for ii, data_val in enumerate(val_loader, 0):\n","            inputs = data_val[0].cuda()\n","            GT = data_val[1].cuda()\n","            with torch.no_grad():\n","                out = model(inputs)\n","            val_loss = criterion(out, GT)\n","            epoch_val_loss += val_loss.item()\n","\n","        if epoch_val_loss < best_val_loss:\n","            best_val_loss = epoch_val_loss\n","            best_epoch = epoch\n","            torch.save({'epoch': epoch,\n","                        'state_dict': model.state_dict(),\n","                        'optimizer': optimizer.state_dict()\n","                        }, os.path.join(model_dir, 'model_best.pth'))\n","\n","        print('[epoch %d Loss: %.4f --- best_epoch %d Best_loss %.4f]' % (\n","            epoch, epoch_val_loss / len(val_loader), best_epoch, best_val_loss / len(val_loader)))\n","\n","        torch.save({'epoch': epoch,\n","                    'state_dict': model.state_dict(),\n","                    'optimizer': optimizer.state_dict()\n","                    }, os.path.join(model_dir, f'model_epoch_{epoch}.pth'))\n","        writer.add_scalar('val/loss', epoch_val_loss / len(val_loader), epoch)\n","    writer.add_scalar('train/loss', epoch_loss / len(train_loader), epoch)\n","writer.close()\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["==> Build folder path\n","==> Build model\n","==> Data preparation\n","==> Training details:\n","------------------------------------------------------------------------------\n","    Network:          MLP_128_256_128\n","    Training data:      97000\n","    Validation data:    1000\n","    Start/End epochs:   1~151\n","    Batch sizes:        100\n","    Learning rate:      0.01\n","    GPU:                True\n","------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 391.03it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  1/151]: \tLoss: 50.4994\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 417.43it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  2/151]: \tLoss: 17.9353\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 417.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  3/151]: \tLoss: 11.8133\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 425.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  4/151]: \tLoss: 9.1700\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 413.21it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  5/151]: \tLoss: 8.3329\t\n","------------------------------------------------------------------\n","[epoch 5 Loss: 5.1111 --- best_epoch 5 Best_loss 5.1111]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 411.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  6/151]: \tLoss: 7.0081\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 411.78it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  7/151]: \tLoss: 6.3856\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 411.18it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  8/151]: \tLoss: 5.8778\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 406.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  9/151]: \tLoss: 5.4005\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 405.74it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 10/151]: \tLoss: 5.2744\t\n","------------------------------------------------------------------\n","[epoch 10 Loss: 6.0943 --- best_epoch 5 Best_loss 5.1111]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 402.98it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 11/151]: \tLoss: 4.8501\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 414.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 12/151]: \tLoss: 5.0590\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 412.34it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 13/151]: \tLoss: 4.4102\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 412.14it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 14/151]: \tLoss: 4.1393\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 414.21it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 15/151]: \tLoss: 3.9969\t\n","------------------------------------------------------------------\n","[epoch 15 Loss: 2.9425 --- best_epoch 15 Best_loss 2.9425]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 413.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 16/151]: \tLoss: 3.9247\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 410.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 17/151]: \tLoss: 4.1873\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 398.71it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 18/151]: \tLoss: 3.8390\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 405.74it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 19/151]: \tLoss: 3.6241\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 409.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 20/151]: \tLoss: 3.6534\t\n","------------------------------------------------------------------\n","[epoch 20 Loss: 4.7140 --- best_epoch 15 Best_loss 2.9425]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 409.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 21/151]: \tLoss: 3.3186\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 408.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 22/151]: \tLoss: 3.4659\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 409.14it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 23/151]: \tLoss: 3.2141\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 409.29it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 24/151]: \tLoss: 3.1347\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 403.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 25/151]: \tLoss: 3.4031\t\n","------------------------------------------------------------------\n","[epoch 25 Loss: 3.2460 --- best_epoch 15 Best_loss 2.9425]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 403.12it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 26/151]: \tLoss: 2.8960\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 408.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 27/151]: \tLoss: 3.2828\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 422.42it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 28/151]: \tLoss: 2.9133\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 406.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 29/151]: \tLoss: 3.0821\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 415.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 30/151]: \tLoss: 3.0494\t\n","------------------------------------------------------------------\n","[epoch 30 Loss: 2.3175 --- best_epoch 30 Best_loss 2.3175]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 413.26it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [ 31/151]: \tLoss: 2.8550\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|███████▎                      | 238/970 [00:00<00:01, 391.51it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-bbf0026498c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"eO1v-JbJWIIU","executionInfo":{"status":"ok","timestamp":1629985803694,"user_tz":-480,"elapsed":309,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}}},"source":["#-------------------------Testing parameter setting-------------------------#\n","train_dir = '/content/train.csv'  #訓練csv位置(得到normalization matrix用)\n","test_dir = '/content/test.csv'   #想測試的csv位置\n","result_dir = '/content'       #結果csv資料夾位置\n","model_weights = '/content/model_best.pth' #模型權重檔\n","gpus = True"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-cJfi8wU0sb","executionInfo":{"status":"ok","timestamp":1629985885586,"user_tz":-480,"elapsed":268,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}},"outputId":"504ce9ac-8ece-49df-cd78-7b0317a2276c"},"source":["#--------------------------------Start testing !----------------------------#\n","import os\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import DataLoader\n","\n","model = MLP(n_inputs=13, hidden_layer1=128, hidden_layer2=256, hidden_layer3=128)\n","# 13, 128, 128, 64, 1\n","\n","torch.multiprocessing.freeze_support()\n","\n","load_checkpoint(model, model_weights)\n","model.cuda()\n","model.eval()\n","\n","test_dataset = TestDataset(train_dir, test_dir)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=0)\n","\n","results = []\n","score = 0\n","max_single_score = 0\n","print('===> Start testing~~')\n","with torch.no_grad():\n","    for ii, data_test in enumerate(tqdm(test_loader, ncols=50, leave=True), 0):\n","        torch.cuda.ipc_collect()\n","        torch.cuda.empty_cache()\n","\n","        input_ = data_test[0].cuda()\n","        answer = data_test[1]\n","        file_names = data_test[2]\n","\n","        predict = model(input_)\n","        predict = predict.cpu().numpy()\n","\n","        for batch in range(len(predict)):\n","            results.append([file_names[batch].item(), predict[batch].item()])\n","            single_score = abs(predict[batch].item() - answer[batch].item())\n","            if single_score < 10:\n","                score += 1\n","            if single_score > max_single_score:\n","                max_single_score = single_score\n","\n","write_csv(data=results, csv_path=result_dir, save_name='results')\n","\n","x = score/len(test_dataset)\n","y = max_single_score\n","score_A = calculate_score_A(x)\n","score_B = calculate_score_B(y)\n","total_score = score_A + score_B\n","print('===> Finish writing csv data!')\n","\n","print(f'''\n","Result: \n","----------------------------------\n","    Score A (70): {score_A}\n","    Score B (30): {score_B}\n","    Total  (100): {total_score}\n","''')\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["===> Start testing~~\n"],"name":"stdout"},{"output_type":"stream","text":["100%|███████████| 72/72 [00:00<00:00, 2482.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["===> Finish writing csv data!\n","\n","Result: \n","----------------------------------\n","    Score A (70): 70.0\n","    Score B (30): 25\n","    Total  (100): 95.0\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}