{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMBD2021-MLP regression.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ptIRp1GUFzZKqgbjW02DRAhRqPf6wjGe","authorship_tag":"ABX9TyMu2q58YIJfExDYbaOvCG/F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"moip0Ehh6g1T"},"source":["# model utils -> 和模型相關的函式\n","import torch\n","import os\n","from collections import OrderedDict\n","def freeze(model):\n","    for p in model.parameters():\n","        p.requires_grad = False\n","def unfreeze(model):\n","    for p in model.parameters():\n","        p.requires_grad = True\n","def is_frozen(model):\n","    x = [p.requires_grad for p in model.parameters()]\n","    return not all(x)\n","def save_checkpoint(model_dir, state, session):\n","    epoch = state['epoch']\n","    model_out_path = os.path.join(model_dir, \"model_epoch_{}_{}.pth\".format(epoch, session))\n","    torch.save(state, model_out_path)\n","def load_checkpoint(model, weights):\n","    checkpoint = torch.load(weights)\n","    try:\n","        model.load_state_dict(checkpoint[\"state_dict\"])\n","    except:\n","        state_dict = checkpoint[\"state_dict\"]\n","        new_state_dict = OrderedDict()\n","        for k, v in state_dict.items():\n","            name = k[7:]  # remove `module.`\n","            new_state_dict[name] = v\n","        model.load_state_dict(new_state_dict)\n","def load_start_epoch(weights):\n","    checkpoint = torch.load(weights)\n","    epoch = checkpoint[\"epoch\"]\n","    return epoch\n","def load_optim(optimizer, weights):\n","    checkpoint = torch.load(weights)\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # for p in optimizer.param_groups: lr = p['lr']\n","    # return lr\n","def network_parameters(nets):\n","    num_params = sum(param.numel() for param in nets.parameters())\n","    return num_params   # /1e6\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slnM8tul8VoQ"},"source":["# dir -> 和路徑有關的函式\n","import os\n","from natsort import natsorted\n","from glob import glob\n","\n","\n","def mkdirs(paths):\n","    if isinstance(paths, list) and not isinstance(paths, str):\n","        for path in paths:\n","            mkdir(path)\n","    else:\n","        mkdir(paths)\n","\n","\n","def mkdir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\n","def get_last_path(path, session):\n","    x = natsorted(glob(os.path.join(path, '*%s' % session)))[-1]\n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSzcigCNVC78"},"source":["# csv -> 和csv有關的函式\n","import csv\n","import os\n","\n","\n","def write_csv(data=None, csv_path=None, save_name='result'):\n","    if not os.path.exists(csv_path):\n","        os.mkdir(csv_path)\n","    headerList = ['Number', 'Predict']\n","    with open(csv_path + \"/\" + save_name + '.csv', 'w', newline='') as f:\n","        w = csv.writer(f)\n","        dw = csv.DictWriter(f, delimiter=',', fieldnames=headerList)\n","        dw.writeheader()\n","        for i in range(len(data)):\n","            name = data[i][0]\n","            predict = data[i][1]\n","            w.writerow([name, predict])\n","    f.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpOFRpIGVMAs"},"source":["# score -> 和分數計算有關的函式\n","\n","def calculate_score_A(x):\n","    return x * 70\n","\n","\n","def calculate_score_B(y):\n","    score_B = 0\n","    if y <= 5:\n","        score_B = 30\n","    elif 5 < y <= 7.5:\n","        score_B = 25\n","    elif 5 < y <= 7.5:\n","        score_B = 20\n","    elif 7.5 < y <= 10:\n","        score_B = 25\n","    elif 10 < y <= 12.5:\n","        score_B = 17.5\n","    elif 12.5 < y <= 15:\n","        score_B = 15\n","    elif 15 < y <= 17.5:\n","        score_B = 10\n","    elif 15 < y <= 17.5:\n","        score_B = 5\n","    elif y > 20:\n","        score_B = 0\n","    return score_B\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hA5hiJeV63Ks"},"source":["# dataset.py -> 整理data，並換為可訓練資料(Normalization)\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import StandardScaler\n","\n","# dataset definition\n","class TrainDataset(Dataset):\n","\n","    # load the dataset\n","    def __init__(self, train_path=None):\n","        # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","        train_out = pd.read_csv(train_path)\n","        inputs = train_out.iloc[:, 1:14].values\n","        outputs = train_out.iloc[:, 14].values\n","\n","        # feature scaling\n","        sc = StandardScaler()\n","        inputs_train = sc.fit_transform(inputs) # sc.fit_transform()\n","        outputs_train = outputs\n","\n","\n","        # 轉成張量(tensor)\n","        self.inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n","        self.outputs_train = torch.tensor(outputs_train, dtype=torch.float32).view(-1, 1)\n","\n","    def __len__(self):\n","        return len(self.outputs_train)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs_train[idx], self.outputs_train[idx]\n","\n","\n","class ValDataset(Dataset):\n","\n","    # load the dataset\n","    def __init__(self, train_path=None, val_path=None):\n","        # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","        train_out = pd.read_csv(train_path)\n","        train_total = train_out.iloc[:, 1:14].values\n","\n","        val_out = pd.read_csv(val_path)\n","        inputs = val_out.iloc[:, 1:14].values\n","        outputs = val_out.iloc[:, 14].values\n","\n","        # feature scaling\n","        sc = StandardScaler()\n","        matrix = sc.fit_transform(train_total)\n","        inputs_train = sc.transform(inputs)\n","        outputs_train = outputs\n","\n","        # 轉成張量(tensor)\n","        self.inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n","        self.outputs_train = torch.tensor(outputs_train, dtype=torch.float32).view(-1, 1)\n","\n","    class ValDataset(Dataset):\n","\n","        # load the dataset\n","        def __init__(self, train_path=None, val_path=None):\n","            # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","            train_out = pd.read_csv(train_path)\n","            train_total = train_out.iloc[:, 1:14].values\n","\n","            val_out = pd.read_csv(val_path)\n","            inputs = val_out.iloc[:, 1:14].values\n","            outputs = val_out.iloc[:, 14].values\n","\n","            # feature scaling\n","            sc = StandardScaler()\n","            matrix = sc.fit_transform(train_total)\n","            inputs_train = sc.transform(inputs)\n","            outputs_train = outputs\n","\n","            # 轉成張量(tensor)\n","            self.inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n","            self.outputs_train = torch.tensor(outputs_train, dtype=torch.float32).view(-1, 1)\n","\n","    def __len__(self):\n","        return len(self.outputs_train)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs_train[idx], self.outputs_train[idx]\n","\n","\n","class TestDataset(Dataset):\n","\n","    # load the dataset\n","    def __init__(self, train_path=None, test_path=None):\n","        # 讀csv檔併分割訓練資料欄位()與答案欄位()\n","        train_out = pd.read_csv(train_path)\n","        train_total = train_out.iloc[:, 1:14].values\n","\n","        test_out = pd.read_csv(test_path)\n","        inputs = test_out.iloc[:, 1:14].values\n","        outputs = test_out.iloc[:, 14].values\n","        file_names = test_out.iloc[:, 0].values\n","\n","        # feature scaling\n","        sc = StandardScaler()\n","        matrix = sc.fit_transform(train_total)\n","        inputs_test = sc.transform(inputs)\n","        outputs_test = outputs\n","        names = file_names\n","\n","        # 轉成張量(tensor)\n","        self.inputs_test = torch.tensor(inputs_test, dtype=torch.float32)\n","        self.outputs_test = torch.tensor(outputs_test, dtype=torch.float32).view(-1, 1)\n","        self.file_name = torch.tensor(names, dtype=torch.int32).view(-1, 1)\n","\n","    def __len__(self):\n","        return len(self.inputs_test)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs_test[idx], self.outputs_test[idx], self.file_name[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgMR8Jgm7aTM"},"source":["# model.py -> 模型架構\n","import torch.nn as nn\n","\n","\n","# model definition\n","class MLP(nn.Module):\n","    # define model elements\n","    def __init__(self, n_inputs, hidden_layer1, hidden_layer2, hidden_layer3):\n","        super(MLP, self).__init__()\n","        self.layer_1 = nn.Linear(n_inputs, hidden_layer1)\n","        self.act1 = nn.PReLU()\n","        self.layer_2 = nn.Linear(hidden_layer1, hidden_layer2)\n","        self.act2 = nn.PReLU()\n","        self.layer_3 = nn.Linear(hidden_layer2, hidden_layer3)\n","        self.act3 = nn.PReLU()\n","        self.layer_4 = nn.Linear(hidden_layer3, 1)\n","        self.act4 = nn.LeakyReLU(0.2)\n","\n","    # forward propagate input\n","    def forward(self, x):\n","        x = self.act1(self.layer_1(x))\n","        x = self.act2(self.layer_2(x))\n","        x = self.act3(self.layer_3(x))\n","        x = self.act4(self.layer_4(x))\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJ6iTj5I7rW-","executionInfo":{"status":"ok","timestamp":1629979932826,"user_tz":-480,"elapsed":2195,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}},"outputId":"47fbb177-a7fc-4c5e-965d-64617785a79b"},"source":["# install tensorboardX\n","!pip install tensorboardx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (2.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p6PKTfTmPyKc"},"source":["# Training parameter setting\n","\n","Network = 'MLP_128_256_128' #網路名字\n","EPOCH = 10\n","LR = 0.01\n","GPU = True\n","BATCH = 100\n","VAL_AFTER_EVERY = 5 #每幾個epoch存一次模型\n","TRAIN_DIR = '/content/train.csv' # path to training data\n","VAL_DIR = '/content/val.csv' # path to validation data\n","SAVE_DIR = '/content' # path to save models and images\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYxSJ4pu6I0g","outputId":"5c2313ca-b419-4ca8-e566-5dbc0355bffb"},"source":["#--------------------------------Start training !----------------------------#\n","import yaml\n","import os\n","import torch\n","\n","torch.backends.cudnn.benchmark = True\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","\n","import random\n","import time\n","import numpy as np\n","\n","from tqdm import tqdm\n","from tensorboardX import SummaryWriter\n","\n","## Set Seeds\n","random.seed(1234)\n","np.random.seed(1234)\n","torch.manual_seed(1234)\n","torch.cuda.manual_seed_all(1234)\n","\n","## Model and log path direction\n","print('==> Build folder path')\n","start_epoch = 1\n","network_dir = os.path.join(SAVE_DIR, Network)\n","mkdir(network_dir)\n","save_dir = os.path.join(network_dir)\n","mkdir(save_dir)\n","model_dir = os.path.join(network_dir, 'models')\n","mkdir(model_dir)\n","log_dir = os.path.join(network_dir, 'log')\n","mkdir(log_dir)\n","train_dir = TRAIN_DIR\n","val_dir = VAL_DIR\n","\n","writer = SummaryWriter(log_dir=log_dir, filename_suffix=f'_log')\n","\n","# GPU device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Model\n","print('==> Build model')\n","model = MLP(n_inputs=13, hidden_layer1=128, hidden_layer2=128, hidden_layer3=64)\n","if GPU:\n","    model.to(device=device)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","# Loss function\n","criterion = nn.L1Loss()\n","\n","# DataLoaders\n","print('==> Data preparation')\n","train_dataset = TrainDataset(train_dir)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH,\n","                          shuffle=True, num_workers=0, drop_last=False, pin_memory=False)\n","val_dataset = ValDataset(train_dir, val_dir)\n","val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH, shuffle=False, num_workers=0,\n","                        drop_last=False, pin_memory=False)\n","\n","print(f'''==> Training details:\n","------------------------------------------------------------------------------\n","    Network:          {Network}\n","    Training data:      {len(train_dataset)}\n","    Validation data:    {len(val_dataset)}\n","    Start/End epochs:   {str(start_epoch) + '~' + str(EPOCH + 1)}\n","    Batch sizes:        {BATCH}\n","    Learning rate:      {LR}\n","    GPU:                {GPU}''')\n","print('------------------------------------------------------------------------------')\n","\n","# train\n","best_val_loss = 10000\n","best_epoch = 0\n","for epoch in range(start_epoch, EPOCH + 1):\n","    epoch_loss = 0\n","    model.train()\n","    for i, data in enumerate(tqdm(train_loader, ncols=70, total=len(train_loader), leave=True), 0):\n","\n","        for param in model.parameters():\n","            param.grad = None\n","\n","        inputs = data[0].cuda()\n","        GT = data[1].cuda()\n","        out = model(inputs)\n","        loss = criterion(out, GT)\n","\n","        # optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(' Epoch [{:3d}/{}]: \\tLoss: {:.4f}\\t'.format(epoch, EPOCH + 1, epoch_loss / len(train_loader)))\n","    print('------------------------------------------------------------------')\n","    torch.save({'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': optimizer.state_dict()\n","                }, os.path.join(model_dir, 'model_latest.pth'))\n","\n","    # validation (evaluation)\n","    if epoch % VAL_AFTER_EVERY == 0:\n","        model.eval()\n","        epoch_val_loss = 0\n","        for ii, data_val in enumerate(val_loader, 0):\n","            inputs = data_val[0].cuda()\n","            GT = data_val[1].cuda()\n","            with torch.no_grad():\n","                out = model(inputs)\n","            val_loss = criterion(out, GT)\n","            epoch_val_loss += val_loss.item()\n","\n","        if epoch_val_loss < best_val_loss:\n","            best_val_loss = epoch_val_loss\n","            best_epoch = epoch\n","            torch.save({'epoch': epoch,\n","                        'state_dict': model.state_dict(),\n","                        'optimizer': optimizer.state_dict()\n","                        }, os.path.join(model_dir, 'model_best.pth'))\n","\n","        print('[epoch %d Loss: %.4f --- best_epoch %d Best_loss %.4f]' % (\n","            epoch, epoch_val_loss / len(val_loader), best_epoch, best_val_loss / len(val_loader)))\n","\n","        torch.save({'epoch': epoch,\n","                    'state_dict': model.state_dict(),\n","                    'optimizer': optimizer.state_dict()\n","                    }, os.path.join(model_dir, f'model_epoch_{epoch}.pth'))\n","        writer.add_scalar('val/loss', epoch_val_loss / len(val_loader), epoch)\n","    writer.add_scalar('train/loss', epoch_loss / len(train_loader), epoch)\n","writer.close()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==> Build folder path\n","==> Build model\n","==> Data preparation\n","==> Training details:\n","------------------------------------------------------------------------------\n","    Network:          MLP_128_256_128\n","    Training data:      97000\n","    Validation data:    1000\n","    Start/End epochs:   1~11\n","    Batch sizes:        100\n","    Learning rate:      0.01\n","    GPU:                True\n","------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 392.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  1/11]: \tLoss: 4.6942\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 379.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  2/11]: \tLoss: 2.9401\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 407.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  3/11]: \tLoss: 2.4107\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 408.40it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  4/11]: \tLoss: 2.0777\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 408.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  5/11]: \tLoss: 1.8547\t\n","------------------------------------------------------------------\n","[epoch 5 Loss: 1.4942 --- best_epoch 5 Best_loss 1.4942]\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 406.27it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  6/11]: \tLoss: 1.6287\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 401.53it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  7/11]: \tLoss: 1.5285\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 405.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  8/11]: \tLoss: 1.4531\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████████████████████████| 970/970 [00:02<00:00, 401.26it/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch [  9/11]: \tLoss: 1.3274\t\n","------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":[" 93%|███████████████████████████▉  | 904/970 [00:02<00:00, 404.88it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eO1v-JbJWIIU"},"source":["# Testing parameter setting\n","train_dir = '/content/train.csv'\n","test_dir = '/content/val.csv'\n","result_dir = '/content'\n","model_weights = '/content/model_best.pth'\n","gpus = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-cJfi8wU0sb","executionInfo":{"status":"ok","timestamp":1629980112271,"user_tz":-480,"elapsed":670,"user":{"displayName":"范植貿","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbrXQZx9mkeoC2BBMh2ZngEFvkvkktrvA_pcxiBg=s64","userId":"06692656114396923736"}},"outputId":"1ea10696-6af2-40a2-9bd8-6f4426b0816b"},"source":["#--------------------------------Start testing !----------------------------#\n","import os\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","model = MLP(n_inputs=13, hidden_layer1=128, hidden_layer2=256, hidden_layer3=128)\n","# 13, 128, 128, 64, 1\n","\n","\n","torch.multiprocessing.freeze_support()\n","\n","load_checkpoint(model, model_weights)\n","model.cuda()\n","model.eval()\n","\n","test_dataset = TestDataset(train_dir, test_dir)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=0)\n","\n","results = []\n","score = 0\n","max_single_score = 0\n","print('===> Start testing~~')\n","with torch.no_grad():\n","    for ii, data_test in enumerate(tqdm(test_loader, ncols=70, leave=True), 0):\n","        torch.cuda.ipc_collect()\n","        torch.cuda.empty_cache()\n","\n","        input_ = data_test[0].cuda()\n","        answer = data_test[1]\n","        file_names = data_test[2]\n","\n","        predict = model(input_)\n","        predict = predict.cpu().numpy()\n","\n","        for batch in range(len(predict)):\n","            results.append([file_names[batch].item(), predict[batch].item()])\n","            single_score = abs(predict[batch].item() - answer[batch].item())\n","            if single_score < 10:\n","                score += 1\n","            if single_score > max_single_score:\n","                max_single_score = single_score\n","\n","write_csv(data=results, csv_path=result_dir, save_name='results')\n","\n","x = score/len(test_dataset)\n","y = max_single_score\n","score_A = calculate_score_A(x)\n","score_B = calculate_score_B(y)\n","total_score = score_A + score_B\n","print('===> Finish writing csv data!')\n","\n","print(f'''\n","Result: \n","----------------------------------\n","    Score A (70): {score_A}\n","    Score B (30): {score_B}\n","    Total  (100): {total_score}\n","''')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["===> Start testing~~\n"],"name":"stdout"},{"output_type":"stream","text":["100%|███████████████████████████| 1000/1000 [00:00<00:00, 2763.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["===> Finish writing csv data!\n","\n","Result: \n","----------------------------------\n","    Score A (70): 70.0\n","    Score B (30): 25\n","    Total  (100): 95.0\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}